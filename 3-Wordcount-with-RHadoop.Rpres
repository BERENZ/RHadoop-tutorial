Wordcount with mapreduce()
========================================================
author: Andrie de Vries & Simon Field
date: 2015-07-01, UseR!2015
width: 1680
height: 1050
css: css/custom.css

rhdfs
=====
type: section

rhdfs function overview
=======================

* Initialize
  - `hdfs.init()`
  - `hdfs.defaults()`
* File and directory manipulation
  - `hdfs.ls()`
  - `hdfs.delete()`
  - `hdfs.mkdir()`
  - `hdfs.exists()`
* Copy and move from local <-> HDFS
  - `hdfs.put()`
  - `hdfs.get()`

***

* Manipulate files within HDFS
  - `hdfs.copy()`
  - `hdfs.move()`
  - `hdfs.rename()`
* Reading files directly from HDFS
  - `hdfs.file()`
  - `hdfs.read()`
  - `hdfs.write()`
  - `hdfs.flush()`
  - `hdfs.seek()`
  - `hdfs.tell(con)`
  - `hdfs.close()`
  - `hdfs.line.reader()`
  -  `hdfs.read.text.file()`



Word count
==========

![](images/xkcd-wordcount.png)

***

* Word count is the archetypal `hello world!` in Hadoop
* The mapper splits text into individual words and counts occurrences
* Reducer computes total across all mappers



Demo 2
======

```{r demo-2-read, cache=FALSE, include=FALSE}
read_chunk("demo/02-intro-tapply.R")
```

```{r load-packages, include=FALSE}
```

```{r R}
```

Demo 2
======

```{r rmr}
```

Demo 2
======

```{r demo-2.3}
```


End
===
type: section

Thank you.
