hdfs
What is the Hadoop dfs?
Distributed file system
Automatic redundancy
Designed for large volumes of data, written once, read frequently
Individual files get split across nodes
rhdfs function overview
File & directory manipulation
hdfs.ls() 
hdfs.delete()
hdfs.mkdir()
hdfs.exists()
Copy, move & rename files within HDFS
hdfs.copy(), hdfs.move(), hdfs.rename()
Copy, move & rename files from local <-> HDFS
hdfs.put(), hdfs.get()
Reading files directly from HDFS
hdfs.file(), hdfs.read(), hdfs.write(), hdfs.flush()
hdfs.seek(), hdfs.tell(con), hdfs.close()
hdfs.line.reader(), hdfs.read.text.file()
Misc.
hdfs.init(), hdfs.defaults()
Exercise-3.R
Download a book from Project Gutenberg to the local file system. Then put the file in hdfs using hdfs.put()
Word count
Word count
A word count is the archetypal “hello world!” in Hadoop
The mapper splits text into individual words and counts occurrences
Reducer computes total across all mappers
Map Reduce – Word Count Example
Exercise-4.R
Write a standard R script to count the number of words in the ebook you downloaded (use the local file system).

Don’t use mapreduce just yet!
Exercise-5.R
Now complete the wordcount using mapreduce()
